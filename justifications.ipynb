{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e465f3a-a8c3-4d0c-a45c-98df770d3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed0837-95fa-46b8-b0fa-3967576dbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d4677-0590-4f21-8bf0-7206f2574369",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields\"></a>\n",
    "# Meta-fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b773eb-1825-4d63-a3ca-b5bdf752e28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"anchor_metafields_dataseta\"></a>\n",
    "## [DatasetA](#anchor_dataseta) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79deda-d06d-4686-bd13-ff30749a7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The path to the full dataset with all relevant fields\n",
    "DATASETA_PATH = \"data-compas/compas-scores-two-years.csv\"\n",
    "\n",
    "### The decision source to explain, needs to be 0/1 split\n",
    "DATASETA_DECISION = \"2YCOMPAS\" # Original dataset \"low-risk/high-risk\" threshold split\n",
    "# DATASETA_DECISION = \"RACIST\" # Explicitly judging by race group only, [TODO] with the same accuracy vs. same low/high risk split as original?\n",
    "\n",
    "### Name of column to use in justification\n",
    "DATASETA_DECISION_COLNAME = \"ncol_decision\"\n",
    "\n",
    "### Seed to use in 'ref'(reference,train)/'evl'(evaluate,test) split sampling\n",
    "DATASETA_SPLIT_SEED = 1\n",
    "### Ratio of 'train'/'test' split sampling\n",
    "DATASETA_SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a040460-65fa-46b6-9488-feda80548480",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_datasetb\"></a>\n",
    "## [DatasetB](#anchor_datasetb) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906f6cb-ba7b-49df-bc89-2a1300146e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Maximum number of factors to include in the whole-dataset evaluation\n",
    "# MAX_RATEXPL_FACTORS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3db0da-fb20-45c3-8bb7-af65444de85c",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_explain\"></a>\n",
    "## [Explain](#anchor_explain) fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493143d-0b47-4c11-889a-3fa129ae5155",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_evaluate\"></a>\n",
    "## [Evaluate](#anchor_evaluate) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f60be3-9058-450d-979a-99d3f4d273b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Fields accessible to use in rationalization?\n",
    "# ### ... some of which are VERY unfair and/or illegal\n",
    "# ACCESSIBLE_FIELDS = [\n",
    "#     # 'sex', # >:(\n",
    "#     # 'race', # >:(\n",
    "#     # 'age', # >:(\n",
    "#     # 'age_cat',\n",
    "#     'juv_fel_count',\n",
    "#     'juv_misd_count',\n",
    "#     'juv_other_count',\n",
    "#     'priors_count',\n",
    "#     'c_charge_degree',\n",
    "#     'c_charge_desc',\n",
    "# ]\n",
    "\n",
    "# ### Field that is the source of what we are rationalizing based on\n",
    "# TRUERESULT_FIELD = 'two_year_recid'\n",
    "# # TRUERESULT_FIELD = 'is_recid'\n",
    "# ### Field we are trying to rationalize why it could be 1 vs 0\n",
    "# # JUSTIFYING_FIELD = 'is_recid' # not going to use 'is_violent_recid' much here\n",
    "# JUSTIFYING_FIELD = 'two_year_recid'\n",
    "# ### Field that is 1 if justifying==trueresult, otherwise 0.\n",
    "# ### This is DEFINED BY ME AND NOT IN THE ORIGINAL DATA\n",
    "# WASCORRECT_FIELD = 'pred_accurate'\n",
    "\n",
    "# ### Threshold for confidence range\n",
    "# CONF_ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd025fc9-bdb4-40ee-b928-f3b7d9f058d1",
   "metadata": {},
   "source": [
    "<a id=\"anchor_dataseta\"></a>\n",
    "# DatasetA: what decisions were made?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_dataseta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda5767-d894-43cc-97a8-54739206092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n",
    "def dataseta_compas_filter(df):\n",
    "    df = df[df[\"days_b_screening_arrest\"] >= -30]\n",
    "    df = df[df[\"days_b_screening_arrest\"] <= 30]\n",
    "    df = df[df[\"is_recid\"] != -1]\n",
    "    df = df[df[\"c_charge_degree\"] != \"O\"]\n",
    "    df = df[df[\"score_text\"] != \"N/A\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df1f01-5ae4-424b-a33d-e3b67bda7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataseta_decision_compas(df):\n",
    "    return df[\"is_recid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cf732-bffc-4df0-b7c1-9631be08d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataseta_decision_racist(df):\n",
    "    # Match for the number of positive predictions\n",
    "    num_pos_pred = sum(df[\"is_recid\"].values)\n",
    "    # sort by race, apply positives from 1->0 in alphabetic race order\n",
    "    df = df.sort_values(\"race\")\n",
    "    df[\"temp\"] = [\n",
    "        (1 if i<num_pos_pred else 0) \n",
    "        for i in range(len(df))\n",
    "    ]\n",
    "    # re-order to match original dataset index\n",
    "    df = df.sort_index()\n",
    "    return df[\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac869e-2cc3-4bca-9ada-f91748cbfb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "rawsrc_df = pd.read_csv(DATASETA_PATH)\n",
    "\n",
    "# Filter / preprocess the dataset to remove edge cases\n",
    "decisions_df = dataseta_compas_filter(rawsrc_df)\n",
    "\n",
    "# Add model decisions as a custom named column\n",
    "decisions_ref = {\n",
    "    \"2YCOMPAS\": dataseta_decision_compas,\n",
    "    \"RACIST\": dataseta_decision_racist,\n",
    "}\n",
    "decisions_df[DATASETA_DECISION_COLNAME] = decisions_ref[DATASETA_DECISION](decisions_df)\n",
    "\n",
    "# [TODO] backup\n",
    "\n",
    "# Do ref/evl (train/test) split\n",
    "decisions_ref_df = decisions_df.sample(\n",
    "    n=int(DATASETA_SPLIT_RATIO*len(decisions_df)),\n",
    "    random_state=DATASETA_SPLIT_SEED,\n",
    ")\n",
    "decisions_evl_df = decisions_df[\n",
    "    ~decisions_df.index.isin(decisions_ref_df.index)\n",
    "]\n",
    "\n",
    "# Print preview of the dataset\n",
    "decisions_df.shape\n",
    "decisions_df[:5]\n",
    "decisions_ref_df.shape\n",
    "decisions_evl_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b9e59-b0cb-4d5d-9597-533980fcb3d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"anchor_datasetb\"></a>\n",
    "# DatasetB: what explanations could be used?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_datasetb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521be8c-82f2-4910-8b7b-295410c72a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16beb5a0-2b7b-4b14-9588-dbc6d08b1a9e",
   "metadata": {},
   "source": [
    "<a id=\"anchor_explain\"></a>\n",
    "# Explain: what explanations were actually used for each case?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303da4a-fcca-4d7d-ae27-9e6874edb4db",
   "metadata": {},
   "source": [
    "<a id=\"anchor_evaluate\"></a>\n",
    "# Evaluate: what are the faithfulness metrics for a given set of used explanations?\n",
    "\n",
    "[Relevant metafields](anchor_metafields_evaluate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
