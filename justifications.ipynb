{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e465f3a-a8c3-4d0c-a45c-98df770d3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed0837-95fa-46b8-b0fa-3967576dbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d4677-0590-4f21-8bf0-7206f2574369",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields\"></a>\n",
    "# Meta-fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b773eb-1825-4d63-a3ca-b5bdf752e28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"anchor_metafields_dataseta\"></a>\n",
    "## [DatasetA](#anchor_dataseta) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79deda-d06d-4686-bd13-ff30749a7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The path to the full dataset with all relevant fields\n",
    "DATASETA_PATH = \"data-compas/compas-scores-two-years.csv\"\n",
    "\n",
    "### The decision source to explain, needs to be 0/1 split\n",
    "DATASETA_DECISION = \"2YCOMPAS\" # Original dataset \"low-risk/high-risk\" threshold split\n",
    "# DATASETA_DECISION = \"RACIST\" # Explicitly judging by race group only, with the same % low/high split\n",
    "# [TODO] \"RACIST\" with the same accuracy?\n",
    "\n",
    "### Name of column to use for decision made\n",
    "DATASETA_DECISION_COLNAME = \"ncol_decision\"\n",
    "\n",
    "### Name of existing column to use in identification\n",
    "DATASETA_ID_COLNAME = \"id\"\n",
    "\n",
    "### Seed to use in 'ref'(reference,train)/'evl'(evaluate,test) split sampling\n",
    "DATASETA_SPLIT_SEED = 1\n",
    "### Ratio of 'ref'/'evl' split sampling\n",
    "DATASETA_SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a040460-65fa-46b6-9488-feda80548480",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_datasetb\"></a>\n",
    "## [DatasetB](#anchor_datasetb) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906f6cb-ba7b-49df-bc89-2a1300146e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assume that we are always doing \"statistically significant compared to\n",
    "### general population\" type justifications\n",
    "\n",
    "### Maximum number of factors to include in the whole-dataset evaluation\n",
    "DATASETB_MAX_FIELDS = 2\n",
    "### Fields accessible to use in rationalization?\n",
    "### ... some of which are VERY unfair and/or illegal\n",
    "DATASETB_JUSTIF_FIELDS = [\n",
    "    # 'sex', # protected trait\n",
    "    # 'race', # protected trait\n",
    "    # 'age', # protected trait\n",
    "    # 'age_cat',\n",
    "    'juv_fel_count',\n",
    "    'juv_misd_count',\n",
    "    'juv_other_count',\n",
    "    'priors_count',\n",
    "    'c_charge_degree',\n",
    "    'c_charge_desc',\n",
    "]\n",
    "\n",
    "### Name of column to use for justification\n",
    "### Should be a dataset source column name... [TODO] update that if needed\n",
    "DATASETB_EVIDENCE_COLNAME = \"two_year_recid\"\n",
    "\n",
    "### Threshold for confidence range\n",
    "DATASETB_CONF_ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3db0da-fb20-45c3-8bb7-af65444de85c",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_justify\"></a>\n",
    "## [Justify](#anchor_justify) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa460c0b-0b67-4aab-baef-bdcbf61461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method to use for justification assignment\n",
    "JUSTIFY_APPROACH = \"MIN_REFSIZE_AGREE\" # 1. agreeable justification with min ref sample size, 2. 'None'\n",
    "# JUSTIFY_APPROACH = \"MIN_REFSIZE_ANY\" # 1. agreeable justification with min ref sample size, 2. any justification with min ref size, 3. 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493143d-0b47-4c11-889a-3fa129ae5155",
   "metadata": {},
   "source": [
    "<a id=\"anchor_metafields_evaluate\"></a>\n",
    "## [Evaluate](#anchor_evaluate) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8e5aa-fd11-4ee0-932c-79ba9dd66933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd025fc9-bdb4-40ee-b928-f3b7d9f058d1",
   "metadata": {},
   "source": [
    "<a id=\"anchor_dataseta\"></a>\n",
    "# DatasetA: what decisions were made?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_dataseta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda5767-d894-43cc-97a8-54739206092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n",
    "def dataseta_compas_filter(df):\n",
    "    df = df[df[\"days_b_screening_arrest\"] >= -30]\n",
    "    df = df[df[\"days_b_screening_arrest\"] <= 30]\n",
    "    df = df[df[\"is_recid\"] != -1]\n",
    "    df = df[df[\"c_charge_degree\"] != \"O\"]\n",
    "    df = df[df[\"score_text\"] != \"N/A\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df1f01-5ae4-424b-a33d-e3b67bda7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataseta_decision_compas(df):\n",
    "    return df[\"is_recid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722cf732-bffc-4df0-b7c1-9631be08d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataseta_decision_racist(df):\n",
    "    # Match for the number of positive predictions\n",
    "    num_pos_pred = sum(df[\"is_recid\"].values)\n",
    "    # sort by race, apply positives from 1->0 in alphabetic race order\n",
    "    df = df.sort_values(\"race\")\n",
    "    df[\"temp\"] = [\n",
    "        (1 if i<num_pos_pred else 0) \n",
    "        for i in range(len(df))\n",
    "    ]\n",
    "    # re-order to match original dataset index\n",
    "    df = df.sort_index()\n",
    "    return df[\"temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac869e-2cc3-4bca-9ada-f91748cbfb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "rawsrc_df = pd.read_csv(DATASETA_PATH)\n",
    "\n",
    "# Filter / preprocess the dataset to remove edge cases\n",
    "decisions_df = dataseta_compas_filter(rawsrc_df)\n",
    "\n",
    "# Add model decisions as a custom named column\n",
    "decisions_ref = {\n",
    "    \"2YCOMPAS\": dataseta_decision_compas,\n",
    "    \"RACIST\": dataseta_decision_racist,\n",
    "}\n",
    "decisions_df[DATASETA_DECISION_COLNAME] = decisions_ref[DATASETA_DECISION](decisions_df)\n",
    "\n",
    "# [TODO] backup\n",
    "\n",
    "# Do ref/evl (train/test) split\n",
    "decisions_ref_df = decisions_df.sample(\n",
    "    n=int(DATASETA_SPLIT_RATIO*len(decisions_df)),\n",
    "    random_state=DATASETA_SPLIT_SEED,\n",
    ")\n",
    "decisions_evl_df = decisions_df[\n",
    "    ~decisions_df.index.isin(decisions_ref_df.index)\n",
    "]\n",
    "\n",
    "# Print preview of the dataset\n",
    "decisions_df.shape\n",
    "decisions_ref_df.shape\n",
    "decisions_ref_df[:3]\n",
    "decisions_evl_df.shape\n",
    "decisions_evl_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6625698-ad48-4643-a850-361fb35f8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO visualize the dataset itself?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b9e59-b0cb-4d5d-9597-533980fcb3d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"anchor_datasetb\"></a>\n",
    "# DatasetB: what explanations could be used?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_datasetb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d33ab4-901c-4a94-90bb-056f6a93a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true iff the intervals [a1, a2] and [b1, b2] overlap\n",
    "def interval_overlaps(a1, a2, b1, b2):\n",
    "    # Input bounds checking\n",
    "    if a1>a2:\n",
    "        raise ValueError(\"unexpected input bounds: a1>a2\")\n",
    "    if b1>b2:\n",
    "        raise ValueError(\"unexpected input bounds: b1>b2\")\n",
    "    # Actual computation\n",
    "    if a1<b1 and a2<b1:\n",
    "        return False\n",
    "    if a1>b2 and a2>b2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521be8c-82f2-4910-8b7b-295410c72a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import proportion\n",
    "\n",
    "# define a function that takes a ref_df and a evl_df, \n",
    "# list of usable columns, number fields per justification, \n",
    "# and evidence column name, and computes the set of all possible column \n",
    "# field combinations in evl_df, and each of their set significances in ref_df\n",
    "# Be sure to include some calculation of how applicable each justification\n",
    "# is to each case in evl_df\n",
    "def datasetb_popsignificant(\n",
    "    ref_df, evl_df, \n",
    "    justif_fields, num_fields, evidence_colname, conf_alpha, \n",
    "    id_colname\n",
    "):\n",
    "    # compute population uncertainty interval\n",
    "    ref_values = ref_df[evidence_colname]\n",
    "    ref_average = sum(ref_values) / len(ref_values)\n",
    "    ref_interval = proportion.proportion_confint(\n",
    "        sum(ref_values), len(ref_values),\n",
    "        alpha=conf_alpha, method=\"beta\",\n",
    "    )\n",
    "    # calculate every possible combination of justification field types\n",
    "    justif_field_idxs = [[i] for i in range(len(justif_fields))]\n",
    "    for _ in range(num_fields-1):\n",
    "        justif_field_idxs = [\n",
    "            [f+[i] for i in range(len(justif_fields)) if i>f[-1]]\n",
    "            for f in justif_field_idxs\n",
    "        ]\n",
    "        justif_field_idxs = [e for sl in justif_field_idxs for e in sl]\n",
    "    justif_field_types = [\n",
    "        [justif_fields[i] for i in fi]\n",
    "        for fi in justif_field_idxs\n",
    "    ]\n",
    "    justif_df = pd.DataFrame()\n",
    "    justif_relevant_df = pd.DataFrame()\n",
    "    # calculate every usable justification field key (apply types -> evl_df)\n",
    "    # also log which evl_df IDs are relevant to it\n",
    "    for jt in justif_field_types:\n",
    "        for evl_sg_justifval, evl_sg_df in evl_df.groupby(by=jt):\n",
    "            # generate the key...\n",
    "            evl_sg_justifkey = (\n",
    "                tuple(jt), \n",
    "                (evl_sg_justifval,) if len(jt)==1 else tuple(evl_sg_justifval)\n",
    "            )\n",
    "            # pick out relevant ref subgroup\n",
    "            ref_sg_df = ref_df\n",
    "            for i in range(len(evl_sg_justifkey[0])):\n",
    "                ref_sg_df = ref_sg_df[ref_sg_df[evl_sg_justifkey[0][i]] == evl_sg_justifkey[1][i]]\n",
    "            if len(ref_sg_df) == 0:\n",
    "                continue\n",
    "            # compute subgroup uncertainty interval\n",
    "            ref_sg_values = ref_sg_df[evidence_colname]\n",
    "            evl_sg_values = evl_sg_df[evidence_colname]\n",
    "            ref_sg_average = sum(ref_sg_values) / len(ref_sg_values)\n",
    "            evl_sg_average = sum(evl_sg_values) / len(evl_sg_values)\n",
    "            ref_sg_interval = proportion.proportion_confint(\n",
    "                sum(ref_sg_values), len(ref_sg_values),\n",
    "                alpha=conf_alpha, method=\"beta\",\n",
    "            )\n",
    "            # Skip non-significant comparisons\n",
    "            if interval_overlaps(\n",
    "                ref_interval[0], ref_interval[1], \n",
    "                ref_sg_interval[0], ref_sg_interval[1]\n",
    "            ):\n",
    "                continue\n",
    "            # Save justification details, applicability details\n",
    "            # append single row for new justification\n",
    "            justif_df = justif_df.append({\n",
    "                \"justif_key\": evl_sg_justifkey,\n",
    "                \"justif_key_numfields\": len(jt),\n",
    "                \"justif_premise\": 1 if (ref_sg_average>ref_average) else 0,\n",
    "                \"ref_mean\": ref_sg_average,\n",
    "                \"ref_samplesize\": len(ref_sg_df),\n",
    "                \"ref_conf_bot\": ref_sg_interval[0],\n",
    "                \"ref_conf_top\": ref_sg_interval[1],\n",
    "                \"evl_mean\": evl_sg_average,\n",
    "                \"evl_samplesize\": len(evl_sg_df),\n",
    "            }, ignore_index=True)\n",
    "            # concat an entire new temp-df for all relevant key-ID pairings\n",
    "            justif_relevant_df = pd.concat([\n",
    "                justif_relevant_df, \n",
    "                pd.DataFrame({\n",
    "                    \"justif_key\": [evl_sg_justifkey for _ in range(len(evl_sg_df))],\n",
    "                    \"evl_id\": evl_sg_df[id_colname],\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "    return justif_df, justif_relevant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8613e5-7949-4c99-9f40-ab941b5a24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = decisions_df[decisions_df[\"id\"]==2680]\n",
    "temp\n",
    "\n",
    "temp_justif_df = pd.DataFrame()\n",
    "temp_justif_relevant_df = pd.DataFrame()\n",
    "for i_num_fields in range(1, DATASETB_MAX_FIELDS+1):\n",
    "    temp_a, temp_b = datasetb_popsignificant(\n",
    "        decisions_ref_df, temp,\n",
    "        DATASETB_JUSTIF_FIELDS, i_num_fields, DATASETB_EVIDENCE_COLNAME, DATASETB_CONF_ALPHA,\n",
    "        DATASETA_ID_COLNAME\n",
    "    )\n",
    "    temp_justif_df = pd.concat([temp_justif_df, temp_a], ignore_index=True)\n",
    "    temp_justif_relevant_df = pd.concat([temp_justif_relevant_df, temp_b], ignore_index=True)\n",
    "\n",
    "temp_justif_df.shape\n",
    "temp_justif_df[:5]\n",
    "temp_justif_relevant_df.shape\n",
    "temp_justif_relevant_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62993d7-1564-48f0-8d35-48bb7a2cbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "justif_df = pd.DataFrame()\n",
    "justif_relevant_df = pd.DataFrame()\n",
    "for i_num_fields in range(1, DATASETB_MAX_FIELDS+1):\n",
    "    temp_a, temp_b = datasetb_popsignificant(\n",
    "        decisions_ref_df, decisions_evl_df,\n",
    "        DATASETB_JUSTIF_FIELDS, i_num_fields, DATASETB_EVIDENCE_COLNAME, DATASETB_CONF_ALPHA,\n",
    "        DATASETA_ID_COLNAME\n",
    "    )\n",
    "    justif_df = pd.concat([justif_df, temp_a], ignore_index=True)\n",
    "    justif_relevant_df = pd.concat([justif_relevant_df, temp_b], ignore_index=True)\n",
    "\n",
    "justif_df.shape\n",
    "justif_df[:5]\n",
    "justif_relevant_df.shape\n",
    "justif_relevant_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048c611-a633-4ce9-9ec3-3f5b986bec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_breakdown = decisions_ref_df[DATASETB_EVIDENCE_COLNAME]\n",
    "print('true recidivism (ref):', sum(temp_breakdown)/len(temp_breakdown))\n",
    "print('true recidivism (ref) range:', proportion.proportion_confint(\n",
    "    sum(temp_breakdown), len(temp_breakdown), alpha=DATASETB_CONF_ALPHA, method='beta'\n",
    "))\n",
    "\n",
    "print()\n",
    "\n",
    "temp_breakdown = decisions_evl_df[DATASETB_EVIDENCE_COLNAME]\n",
    "print('true recidivism (evl):', sum(temp_breakdown)/len(temp_breakdown))\n",
    "print('true recidivism (evl) range:', proportion.proportion_confint(\n",
    "    sum(temp_breakdown), len(temp_breakdown), alpha=DATASETB_CONF_ALPHA, method='beta'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0d902-1983-4b90-b1a6-971eb4d8ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize a sample of the rationalization bounds\n",
    "def vis_ratbounds(ref_df, evl_df, justif_df, justif_rel_df, num_eachside, max_factors):\n",
    "    # change graph size settings\n",
    "    temp_rcparams_figsize = plt.rcParams[\"figure.figsize\"]\n",
    "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "    plt_gapsize = 10\n",
    "    \n",
    "    # visualize the 0-pred cases\n",
    "    for n in range(num_eachside):\n",
    "        target = evl_df[evl_df[DATASETA_DECISION_COLNAME] == 0].sample(n=1).iloc[0]\n",
    "        target_justifs = justif_rel_df[justif_rel_df[\"evl_id\"] == target[DATASETA_ID_COLNAME]]\n",
    "        target_justifs = target_justifs.merge(\n",
    "            justif_df[justif_df[\"justif_key_numfields\"] <= max_factors],\n",
    "            how=\"inner\", on=[\"justif_key\"], validate=\"1:1\",\n",
    "        )\n",
    "        # mean prediction for justifications\n",
    "        target_justifs_grouped = pd.DataFrame()\n",
    "        for i in range(1, max_factors+1):\n",
    "            temp_revised = pd.DataFrame(target_justifs[target_justifs[\"justif_key_numfields\"] <= i])\n",
    "            temp_revised[\"justif_key_numfields\"] = [i for _ in temp_revised[\"justif_key_numfields\"]]\n",
    "            target_justifs_grouped = pd.concat([\n",
    "                target_justifs_grouped,\n",
    "                temp_revised,\n",
    "            ], ignore_index=True)\n",
    "        dots = plt.scatter(\n",
    "            [(2*i)+(2*max_factors*n) for i in target_justifs_grouped[\"justif_key_numfields\"]],\n",
    "            target_justifs_grouped[\"ref_mean\"],\n",
    "            alpha=[(s/max(justif_df[\"ref_samplesize\"]))**0.9 for s in target_justifs_grouped[\"ref_samplesize\"]],\n",
    "        )\n",
    "        # confidence interval gap between most extreme justifications on both sides\n",
    "        for i in range(1, max_factors+1):\n",
    "            target_justifs_i = target_justifs[target_justifs[\"justif_key_numfields\"] <= i]\n",
    "            if len(target_justifs_i) == 0:\n",
    "                continue\n",
    "            range_lo = min(target_justifs_i[\"ref_conf_top\"])\n",
    "            range_hi = max(target_justifs_i[\"ref_conf_bot\"])\n",
    "            if range_lo <= range_hi:\n",
    "                pos_x = (2*i)+(2*max_factors*n)+1\n",
    "                _ = plt.plot(\n",
    "                    [pos_x, pos_x],\n",
    "                    [range_lo, range_hi],\n",
    "                    color=dots.get_facecolors()[0][:-1],\n",
    "                    marker=\"\",\n",
    "                )\n",
    "    # visualize the 1-pred cases\n",
    "    plt_1predgap = (2*max_factors*num_eachside)+plt_gapsize\n",
    "    for n in range(num_eachside):\n",
    "        target = evl_df[evl_df[DATASETA_DECISION_COLNAME] == 1].sample(n=1).iloc[0]\n",
    "        target_justifs = justif_rel_df[justif_rel_df[\"evl_id\"] == target[DATASETA_ID_COLNAME]]\n",
    "        target_justifs = target_justifs.merge(\n",
    "            justif_df[justif_df[\"justif_key_numfields\"] <= max_factors],\n",
    "            how=\"inner\", on=[\"justif_key\"], validate=\"1:1\",\n",
    "        )\n",
    "        # mean prediction for justifications\n",
    "        target_justifs_grouped = pd.DataFrame()\n",
    "        for i in range(1, max_factors+1):\n",
    "            temp_revised = pd.DataFrame(target_justifs[target_justifs[\"justif_key_numfields\"] <= i])\n",
    "            temp_revised[\"justif_key_numfields\"] = [i for _ in temp_revised[\"justif_key_numfields\"]]\n",
    "            target_justifs_grouped = pd.concat([\n",
    "                target_justifs_grouped,\n",
    "                temp_revised,\n",
    "            ], ignore_index=True)\n",
    "        dots = plt.scatter(\n",
    "            [plt_1predgap+(2*i)+(2*max_factors*n) for i in target_justifs_grouped[\"justif_key_numfields\"]],\n",
    "            target_justifs_grouped[\"ref_mean\"],\n",
    "            alpha=[(s/max(justif_df[\"ref_samplesize\"]))**0.9 for s in target_justifs_grouped[\"ref_samplesize\"]],\n",
    "        )\n",
    "        # confidence interval gap between most extreme justifications on both sides\n",
    "        for i in range(1, max_factors+1):\n",
    "            target_justifs_i = target_justifs[target_justifs[\"justif_key_numfields\"] <= i]\n",
    "            if len(target_justifs_i) == 0:\n",
    "                continue\n",
    "            range_lo = min(target_justifs_i[\"ref_conf_top\"])\n",
    "            range_hi = max(target_justifs_i[\"ref_conf_bot\"])\n",
    "            if range_lo <= range_hi:\n",
    "                pos_x = plt_1predgap+(2*i)+(2*max_factors*n)+1\n",
    "                _ = plt.plot(\n",
    "                    [pos_x, pos_x],\n",
    "                    [range_lo, range_hi],\n",
    "                    color=dots.get_facecolors()[0][:-1],\n",
    "                    marker=\"\",\n",
    "                )\n",
    "    # population mean line\n",
    "    breakdown = ref_df[DATASETB_EVIDENCE_COLNAME]\n",
    "    src_range = proportion.proportion_confint(sum(breakdown), len(breakdown), alpha=DATASETB_CONF_ALPHA, method='beta')\n",
    "    plt.axline((1, sum(breakdown)/len(breakdown)), slope=0, alpha=0.8)\n",
    "    plt.fill_between(\n",
    "        [0, 4*max_factors*num_eachside+plt_gapsize+2], \n",
    "        [src_range[1], src_range[1]], \n",
    "        [src_range[0], src_range[0]], \n",
    "        alpha=0.3\n",
    "    )\n",
    "    # 50% line\n",
    "    plt.axline((1, 0.5), slope=0, alpha=0.4)\n",
    "    # Modify axis visibility, add population comparison, add bin tags\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    group_fontdict = {'size':'large'}\n",
    "    ax.text(x=0, y=0.9, s='Predicted low-risk', fontdict=group_fontdict)\n",
    "    ax.text(x=(2*max_factors*num_eachside)+plt_gapsize, y=0.9, s='Predicted high-risk', fontdict=group_fontdict)\n",
    "    ax.set_ylabel('Recidivism rate (dots), confidence gap (lines)', fontdict=group_fontdict)\n",
    "    ax.set_ylim(bottom=0, top=1)\n",
    "    # restore graph size settings\n",
    "    plt.rcParams[\"figure.figsize\"] = temp_rcparams_figsize\n",
    "\n",
    "vis_ratbounds(decisions_ref_df, decisions_evl_df, justif_df, justif_relevant_df, 10, DATASETB_MAX_FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812c7f8-fea0-4294-8244-7ef2788f80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show frequency of justification coverage (1pred/1anti/2both/0none)\n",
    "def vis_coverage(evl_df, justif_df, justif_rel_df):\n",
    "    results = {\n",
    "        \"1supp\": 0,\n",
    "        \"1anti\": 0,\n",
    "        \"2both\": 0,\n",
    "        \"0none\": 0,\n",
    "    }\n",
    "    for evl_id, evl_id_justifs in justif_rel_df.groupby(by=[\"evl_id\"]):\n",
    "        temp = evl_id_justifs.merge(\n",
    "            justif_df,\n",
    "            how=\"inner\", on=[\"justif_key\"], validate=\"1:1\",\n",
    "        )\n",
    "        # calculate the average justification premise for applicable justifications to this case\n",
    "        temp_support = sum(temp[\"justif_premise\"])/len(temp[\"justif_premise\"])\n",
    "        if temp_support==sum(evl_df[evl_df[DATASETA_ID_COLNAME]==evl_id][DATASETA_DECISION_COLNAME]):\n",
    "            results[\"1supp\"] += 1\n",
    "        elif temp_support==0 or temp_support==1:\n",
    "            results[\"1anti\"] += 1\n",
    "        else:\n",
    "            results[\"2both\"] += 1\n",
    "    # count up what cases don't have any usable for them at all\n",
    "    results[\"0none\"] = len(set(evl_df[DATASETA_ID_COLNAME]))-results[\"1supp\"]-results[\"1anti\"]-results[\"2both\"]\n",
    "    return results\n",
    "\n",
    "# All cases\n",
    "vis_coverage(decisions_evl_df, justif_df, justif_relevant_df)\n",
    "# only p=0 cases\n",
    "temp_decisions_df = decisions_evl_df[decisions_evl_df[DATASETA_DECISION_COLNAME]==0]\n",
    "temp_relevant_df = justif_relevant_df[justif_relevant_df[\"evl_id\"].isin(temp_decisions_df[DATASETA_ID_COLNAME])]\n",
    "vis_coverage(temp_decisions_df, justif_df, temp_relevant_df)\n",
    "# only p=1 cases\n",
    "temp_decisions_df = decisions_evl_df[decisions_evl_df[DATASETA_DECISION_COLNAME]==1]\n",
    "temp_relevant_df = justif_relevant_df[justif_relevant_df[\"evl_id\"].isin(temp_decisions_df[DATASETA_ID_COLNAME])]\n",
    "vis_coverage(temp_decisions_df, justif_df, temp_relevant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16beb5a0-2b7b-4b14-9588-dbc6d08b1a9e",
   "metadata": {},
   "source": [
    "<a id=\"anchor_justify\"></a>\n",
    "# Justify: what explanations were actually used for each case?\n",
    "\n",
    "[Relevant metafields](#anchor_metafields_justify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7672cd-937a-4a87-98e2-7fe8aec85c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def justify_approach_minrefsize(evl_df, justif_df, justif_ref_df, must_agree=True):\n",
    "    assigned_df = pd.DataFrame()\n",
    "    for _, evl_row in evl_df.iterrows():\n",
    "        evl_row_id = evl_row[DATASETA_ID_COLNAME]\n",
    "        evl_row_premise = evl_row[DATASETA_DECISION_COLNAME]\n",
    "        usable_je_df = justif_ref_df[justif_ref_df[\"evl_id\"]==evl_row_id]\n",
    "        usable_je_df = usable_je_df.merge(\n",
    "            justif_df,\n",
    "            how=\"inner\", on=[\"justif_key\"], validate=\"1:1\",\n",
    "        )\n",
    "        agreed_usable_je_df = usable_je_df[usable_je_df[\"justif_premise\"] == evl_row_premise]\n",
    "        usable_je_df = usable_je_df.sort_values(\"ref_samplesize\")\n",
    "        agreed_usable_je_df = agreed_usable_je_df.sort_values(\"ref_samplesize\")\n",
    "        if len(agreed_usable_je_df)>0:\n",
    "            assigned_df = assigned_df.append({\n",
    "                \"evl_id\": evl_row_id,\n",
    "                \"justif_key\": agreed_usable_je_df.iloc[0][\"justif_key\"],\n",
    "            }, ignore_index=True)\n",
    "        elif (not must_agree) and len(usable_je_df)>0:\n",
    "            assigned_df = assigned_df.append({\n",
    "                \"evl_id\": evl_row_id,\n",
    "                \"justif_key\": usable_je_df.iloc[0][\"justif_key\"],\n",
    "            }, ignore_index=True)\n",
    "        else:\n",
    "            assigned_df = assigned_df.append({\n",
    "                \"evl_id\": evl_row_id,\n",
    "                \"justif_key\": None,\n",
    "            }, ignore_index=True)\n",
    "    # Check that all cases were assigned some justification\n",
    "    if len(evl_df)!=len(assigned_df):\n",
    "        raise ValueError(f\"missing justifications; evl={evl_df.shape}; assigned={assigned_df.shape}\")\n",
    "    return assigned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e733fff-2ad2-42b3-ba94-132a7a4021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate case justifications\n",
    "justify_ref = {\n",
    "    \"MIN_REFSIZE_AGREE\": lambda e, j, jr: justify_approach_minrefsize(e, j, jr, must_agree=True),\n",
    "    \"MIN_REFSIZE_ANY\": lambda e, j, jr: justify_approach_minrefsize(e, j, jr, must_agree=False),\n",
    "}\n",
    "justif_assigned_df = justify_ref[JUSTIFY_APPROACH](decisions_evl_df, justif_df, justif_relevant_df)\n",
    "\n",
    "justif_assigned_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303da4a-fcca-4d7d-ae27-9e6874edb4db",
   "metadata": {},
   "source": [
    "<a id=\"anchor_evaluate\"></a>\n",
    "# Evaluate: what are the faithfulness metrics for a given set of used justifications?\n",
    "\n",
    "[Relevant metafields](anchor_metafields_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdf2cb-a6e3-4d2f-93ec-522b5773a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_evl_df[:2]\n",
    "justif_df[:2]\n",
    "justif_relevant_df[:2]\n",
    "justif_assigned_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c534e-aac1-43a8-b3b5-57c6ae08dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a set of justifications used for each case in an evl_df,\n",
    "# evaluate based on the metrics defined in (Dasgupta 2022) paper\n",
    "# [https://arxiv.org/pdf/2202.00734.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a5b84-f0fd-482b-bf73-a0a0e52f54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local consistency: \n",
    "# based on a single input case, equals the probability that other cases with\n",
    "# the same explanation assigned have the same decision output as the original\n",
    "# case\n",
    "\n",
    "# Global consistency:\n",
    "# The expected local consistency across the entire input case distribution\n",
    "\n",
    "# IGNORES ALL CASES WHERE NO JUSTIFICATION WAS USABLE\n",
    "\n",
    "def evaluate_dasgupta_faithfulness_consistency(evl_df, justifs_used_df):\n",
    "    score = 0\n",
    "    # Filter out cases where no justification ended up being usable\n",
    "    justifs_used_df = justifs_used_df[justifs_used_df[\"justif_key\"].notnull()]\n",
    "    # group by explanation, weight each group with the number of cases it handles\n",
    "    for justif_key, justif_cases_df in justifs_used_df.groupby(by=[\"justif_key\"], dropna=False):\n",
    "        # calculate the average local consistency for each group\n",
    "        direct_counts = justif_cases_df.merge(\n",
    "            evl_df,\n",
    "            how=\"inner\", left_on=[\"evl_id\"], right_on=[DATASETA_ID_COLNAME], validate=\"1:1\",\n",
    "        )[DATASETA_DECISION_COLNAME].value_counts(normalize=True)\n",
    "        group_consistency = sum([e**2 for e in direct_counts])\n",
    "        score += group_consistency*len(justif_cases_df)\n",
    "    score = score / len(justifs_used_df)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6764799-3c9a-4909-b070-b2befb366eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local sufficiency:\n",
    "# based on a single input case, equals the probability that other cases that\n",
    "# the same explanation is applicable to have the same decision output as the\n",
    "# original case\n",
    "\n",
    "# Global sufficiency:\n",
    "# The expected local sufficiency across the entire input case distribution\n",
    "\n",
    "# IGNORES ALL CASES WHERE NO JUSTIFICATION WAS USABLE\n",
    "\n",
    "def evaluate_dasgupta_faithfulness_sufficiency(evl_df, justifs_used_df, justifs_usable_df):\n",
    "    score = 0\n",
    "    # Filter out cases where no justification ended up being usable\n",
    "    justifs_used_df = justifs_used_df[justifs_used_df[\"justif_key\"].notnull()]\n",
    "    # group by explanation, weight each group with the number of cases it handles\n",
    "    for justif_key, justif_cases_df in justifs_used_df.groupby(by=[\"justif_key\"], dropna=False):\n",
    "        # calculate the average local sufficiency for each group\n",
    "        direct_counts = justif_cases_df.merge(\n",
    "            evl_df,\n",
    "            how=\"inner\", left_on=[\"evl_id\"], right_on=[DATASETA_ID_COLNAME], validate=\"1:1\",\n",
    "        )[DATASETA_DECISION_COLNAME].value_counts(normalize=True)\n",
    "        related_counts = justifs_usable_df[justifs_usable_df[\"justif_key\"]==justif_key].merge(\n",
    "            evl_df,\n",
    "            how=\"inner\", left_on=[\"evl_id\"], right_on=[DATASETA_ID_COLNAME], validate=\"1:1\",\n",
    "        )[DATASETA_DECISION_COLNAME].value_counts(normalize=True)\n",
    "        group_sufficiency = pd.concat([direct_counts, related_counts], axis=1).fillna(0)\n",
    "        group_sufficiency = [[g[0], g[1]] for _, g in group_sufficiency.iterrows()]\n",
    "        group_sufficiency = [l[0]*l[1] for l in group_sufficiency]\n",
    "        group_sufficiency = sum(group_sufficiency)\n",
    "        score += group_sufficiency*len(justif_cases_df)\n",
    "    score = score / len(justifs_used_df)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a75d41-8d5c-432d-b163-8802fc005493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness:\n",
    "# the fraction of test cases whose explanations were unique\n",
    "\n",
    "# IGNORES ALL CASES WHERE NO JUSTIFICATION WAS USABLE\n",
    "\n",
    "def evaluate_dasgupta_faithfulness_uniqueness(evl_df, justifs_used_df):\n",
    "    score = 0\n",
    "    # Filter out cases where no justification ended up being usable\n",
    "    justifs_used_df = justifs_used_df[justifs_used_df[\"justif_key\"].notnull()]\n",
    "    # Count how many cases of unique explanation there were\n",
    "    score += (justifs_used_df[\"justif_key\"].value_counts(dropna=True)==1).sum()\n",
    "    score = score / len(justifs_used_df)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14869d-1f2d-4450-a1cd-d0535cf4989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "justif_assigned_df[justif_assigned_df[\"justif_key\"].notnull()].shape[0]/justif_assigned_df.shape[0]\n",
    "\n",
    "evaluate_dasgupta_faithfulness_consistency(decisions_evl_df, justif_assigned_df)\n",
    "evaluate_dasgupta_faithfulness_sufficiency(decisions_evl_df, justif_assigned_df, justif_relevant_df)\n",
    "evaluate_dasgupta_faithfulness_uniqueness(decisions_evl_df, justif_assigned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a60ec-6eff-4d4a-82e4-8a98559531a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceeea90-3f51-4377-8a9a-c93dd3419f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
